{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project # 8  â€“ Augmented Reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2, numpy as np, os\n",
    "\n",
    "#parameters\n",
    "working_dir = './'\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "org, font, scale, color, thickness, linetype = (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (234,12,123), 2, cv2.LINE_AA\n",
    "#chromakey values\n",
    "h,s,v,h1,s1,v1 = 16,0,64,123,111,187 #green\n",
    "h,s,v,h1,s1,v1 = 0,74,53,68,181,157 #skin tone\n",
    "#amount of data to use\n",
    "data_size = 1000\n",
    "#ratio of training data to test data\n",
    "training_to_test = .75\n",
    "#amount images are scaled down before being fed to keras\n",
    "img_size = 100\n",
    "#image height and width (from the webcam\n",
    "height, width = 480,640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns the region of interest around the largest countour\n",
    "#the accounts for objects not being centred in the frame\n",
    "def bbox(img):\n",
    "    try:\n",
    "        bg = np.zeros((1000,1000), np.uint8)\n",
    "        bg[250:250+480, 250:250+640] = img\n",
    "        _, contours, _  = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        largest_contour = max(contours, key = cv2.contourArea)\n",
    "        rect = cv2.boundingRect(largest_contour)\n",
    "        circ = cv2.minEnclosingCircle(largest_contour)\n",
    "        x,y,w,h = rect\n",
    "        x,y = x+w/2,y+h/2\n",
    "        x,y = x+250, y+250\n",
    "        ddd = 200\n",
    "        return bg[y-ddd:y+ddd, x-ddd:x+ddd]\n",
    "    except: return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finds the largest contour in a list of contours\n",
    "#returns a single contour\n",
    "def largest_contour(contours):\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    return c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finds the center of a contour\n",
    "#takes a single contour\n",
    "#returns (x,y) position of the contour\n",
    "def contour_center(c):\n",
    "    M = cv2.moments(c)\n",
    "    try: center = int(M['m10']/M['m00']), int(M['m01']/M['m00'])\n",
    "    except: center = 0,0\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes image and range\n",
    "#returns parts of image in range\n",
    "def only_color(img, (h,s,v,h1,s1,v1)):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower, upper = np.array([h,s,v]), np.array([h1,s1,v1])\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "    kernel = np.ones((15,15), np.uint)\n",
    "    #mask - cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    res = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return res, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(dimData, images):\n",
    "    images = np.array(images)\n",
    "    images = images.reshape(len(images), dimData)\n",
    "    images = images.astype('float32')\n",
    "    images /=255\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------get train/test data-----------------\n",
    "images, labels = [],[]\n",
    "#iterate through tools\n",
    "tool_name = ''\n",
    "patterns = []\n",
    "tool_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, img = cap.read()\n",
    "    cv2.putText(img, 'enter class name (then enter)', org, font, scale, color, thickness, linetype)\n",
    "    cv2.putText(img, 'press esc when finished', (50,100), font, scale, color, thickness, linetype)\n",
    "    cv2.putText(img, tool_name, (50,300), font, 3, (0,0,255), 5, linetype)\n",
    "    \n",
    "    cv2.line(img, (330,240), (310,240), (234,123,234), 3)\n",
    "    cv2.line(img, (320,250), (320,230), (234,123,234), 3)\n",
    "    cv2.imshow('img', img)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k>10: tool_name += chr(k)\n",
    "    if k == 27: break\n",
    "    #if tool name has been entered, start collecting the data\n",
    "    current = 0\n",
    "    if k == 13:\n",
    "        while current < data_size:\n",
    "            _, img = cap.read()\n",
    "            img, mask = only_color(img, (h,s,v,h1,s1,v1))\n",
    "            mask = bbox(mask)\n",
    "            images.append(cv2.resize(mask, (img_size, img_size)))\n",
    "            labels.append(tool_num)\n",
    "            current += 1\n",
    "            cv2.line(img, (330,240), (310,240), (234,123,234), 3)\n",
    "            cv2.line(img, (320,250), (320,230), (234,123,234), 3)\n",
    "            cv2.putText(img, 'collecting data', org, font, scale, color, thickness, linetype)\n",
    "            cv2.putText(img, 'data for'+tool_name+':' + str(current), (50,100), font, scale, color, thickness, linetype)\n",
    "            #cv2.imshow('img', img)\n",
    "            cv2.imshow('img', mask)\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord('p'): cv2.waitKey(0)\n",
    "            if current == data_size:\n",
    "                patterns.append(tool_name)\n",
    "                tool_name = ''\n",
    "                tool_num += 1\n",
    "                \n",
    "                print tool_num\n",
    "                break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#break data into training and test sets\n",
    "to_train= 0\n",
    "train_images, test_images, train_labels, test_labels = [],[],[],[]\n",
    "for image, label in zip(images, labels):\n",
    "    if to_train<3:\n",
    "        train_images.append(image)\n",
    "        train_labels.append(label)\n",
    "        to_train+=1\n",
    "    else:\n",
    "        test_images.append(image)\n",
    "        test_labels.append(label)\n",
    "        to_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-----------------keras time --> make the model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#flatten data\n",
    "dataDim = np.prod(images[0].shape)\n",
    "train_data  = flatten(dataDim, train_images)\n",
    "test_data = flatten(dataDim, test_images)\n",
    "\n",
    "#change labels to categorical\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "train_labels_one_hot = to_categorical(train_labels)\n",
    "test_labels_one_hot = to_categorical(test_labels)\n",
    "\n",
    "#determine the number of classes\n",
    "classes = np.unique(train_labels)\n",
    "nClasses  = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu', input_shape = (dataDim,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nClasses, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2250 samples, validate on 750 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 1s 450us/step - loss: 0.5310 - acc: 0.7818 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 1s 244us/step - loss: 0.0705 - acc: 0.9769 - val_loss: 6.5299e-05 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 1s 244us/step - loss: 0.0334 - acc: 0.9898 - val_loss: 6.2611e-06 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 1s 246us/step - loss: 0.0173 - acc: 0.9942 - val_loss: 8.0953e-07 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 1s 272us/step - loss: 0.0124 - acc: 0.9956 - val_loss: 3.9657e-07 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 1s 318us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 1.4726e-07 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 1.2342e-07 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 1s 256us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 1.1961e-07 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 1s 246us/step - loss: 0.0042 - acc: 0.9982 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 1s 250us/step - loss: 0.0040 - acc: 0.9982 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 1s 253us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 1s 252us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 1s 249us/step - loss: 0.0012 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 1s 256us/step - loss: 0.0023 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 4.6200e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 1s 252us/step - loss: 2.8146e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 0.0036 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 1s 254us/step - loss: 9.4391e-04 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 1s 259us/step - loss: 3.2355e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 1s 256us/step - loss: 0.0029 - acc: 0.9987 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 1s 251us/step - loss: 7.1702e-04 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 1s 259us/step - loss: 1.6047e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 6.4559e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 2.6253e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 1s 254us/step - loss: 9.8394e-04 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 0.0078 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 1s 252us/step - loss: 8.1438e-04 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 1s 254us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 5.8182e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 1s 263us/step - loss: 7.1573e-04 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 1.0285e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 1s 260us/step - loss: 2.6820e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 5.6117e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 1.9600e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 1s 254us/step - loss: 0.0033 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 1s 255us/step - loss: 9.3873e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 1s 261us/step - loss: 1.2231e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 1s 258us/step - loss: 1.7794e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 8.3790e-04 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 1.9689e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 1s 250us/step - loss: 2.9065e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 1s 251us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 1s 256us/step - loss: 6.4159e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 1s 256us/step - loss: 9.0772e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 1s 252us/step - loss: 4.3206e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 1s 264us/step - loss: 1.5543e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 1s 273us/step - loss: 2.2311e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 1s 257us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_labels_one_hot, batch_size = 256, epochs=50, verbose=1,\n",
    "                    validation_data=(test_data, test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 135us/step\n",
      "Evaluation result on Test Data : Loss = 1.19209289551e-07, accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_prediction(bg,prediction, motion):\n",
    "    idxs = [1,2,3,4,5,6,7,8,9]\n",
    "    for i, pattern, idx in zip(prediction, patterns, idxs):\n",
    "        text = pattern + ' '+str(round(i,3))\n",
    "        scale = i*2\n",
    "        \n",
    "        if motion: scale = .4\n",
    "        if scale<.95: scale = .95\n",
    "        thickness = 1\n",
    "        if scale>1.5: thickness = 2\n",
    "        if scale>1.95: thickness = 4\n",
    "        scale = scale*.75\n",
    "        org, font, color = (350, idx*70), cv2.FONT_HERSHEY_SIMPLEX, (0,0,0)\n",
    "        cv2.putText(bg, text, org, font, scale, color, thickness, cv2.LINE_AA)\n",
    "    return bg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_bg(prediction):\n",
    "    motion = False\n",
    "    bg = np.zeros((1150,1000,3), np.uint8)\n",
    "    idxs = [1,2,3,4,5,6,7,8,9]\n",
    "    for i, pattern, idx in zip(prediction, patterns, idxs):\n",
    "        text = pattern + ' '+str(round(i,3))\n",
    "        scale = i*2\n",
    "        \n",
    "        if motion: scale = .4\n",
    "        if scale<.95: scale = .95\n",
    "        thickness = 1\n",
    "        if scale>1.5: thickness = 2\n",
    "        if scale>1.95: thickness = 4\n",
    "        scale = scale*2\n",
    "        org, font, color = (200, idx*140), cv2.FONT_HERSHEY_SIMPLEX, (12,234,123)\n",
    "        cv2.putText(bg, text, org, font, scale, (255,255,255), 1+thickness, cv2.LINE_AA)\n",
    "    return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#model = load_model('/home/stephen/Desktop/hand_model.h5')\n",
    "dimData = np.prod([img_size, img_size])\n",
    "while True:\n",
    "    _, img= cap.read()\n",
    "    _, mask = only_color(img, (h,s,v,h1,s1,v1))\n",
    "    mask = bbox(mask)\n",
    "    mask = cv2.resize(mask, (img_size, img_size))\n",
    "    cv2.imshow('display', mask)\n",
    "    mask = mask.reshape(dimData)\n",
    "    mask = mask.astype('float32')\n",
    "    mask /=255\n",
    "    prediction = model.predict(mask.reshape(1,dimData))[0].tolist()\n",
    "    img = draw_prediction(img, prediction, False)\n",
    "    display = draw_bg(prediction)\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
